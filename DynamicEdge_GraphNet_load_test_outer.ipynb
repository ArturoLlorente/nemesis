{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.insert(0, '../olympus')\n",
    "sys.path.insert(0, '../graphnet/src')\n",
    "sys.path.insert(0, '../gnn_testbed')\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.1\"\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "#from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import torch\n",
    "import torch_cluster\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from nemesis.event_generation.detector import make_line, generate_noise, Detector, make_triang\n",
    "from nemesis.plotting import plot_event, plot_events, plot_confusion\n",
    "from nemesis.data_handling.utils import event_labelling\n",
    "from nemesis.node_features.feature_generation import generate_features\n",
    "from nemesis.evaluation.evaluation import model_evaluation\n",
    "from nemesis.evaluation.utils import count_parameters\n",
    "from nemesis.models.train import train_model\n",
    "from nemesis.models.gnns import Dynamic_class, DynEdge_modified\n",
    "\n",
    "from torch.nn import Linear, Identity, ReLU, Softmax, Dropout, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import knn_graph, TAGConv, global_mean_pool, global_max_pool, BatchNorm,  global_add_pool, EdgeConv, DynamicEdgeConv\n",
    "\n",
    "from graphnet.models.gnn.dynedge import DynEdge\n",
    "from graphnet.components.layers import DynEdgeConv\n",
    "from graphnet.models.gnn.gnn import GNN\n",
    "from graphnet.models.utils import calculate_xyzt_homophily, calculate_xyz_homophily_POne\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "outpath = \".\"\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(31338)\n",
    "oms_per_line = 20\n",
    "dist_z = 50 # m\n",
    "dark_noise_rate = 16 * 1e-5  # 1/ns\n",
    "side_len = 100 # m\n",
    "pmts_per_module = 16\n",
    "pmt_cath_area_r = 75E-3 / 2 # m\n",
    "module_radius = 0.21 # m\n",
    "v_x = 0\n",
    "efficiency = pmts_per_module * (pmt_cath_area_r)**2 * np.pi / (4*np.pi*module_radius**2)\n",
    "det = make_triang(side_len, oms_per_line, dist_z, dark_noise_rate, rng, efficiency=efficiency, v_x=v_x, buoy_weight=30)\n",
    "module_positions = jnp.asarray(det.module_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#import pickle\n",
    "#import numpy as np\n",
    "#import os\n",
    "#file_idx = range(12)\n",
    "\n",
    "def load_all_events(it):\n",
    "    path_to_events = \"/dss/pone/pone_events/all_events\"\n",
    "    load_array = ['cascades_15000ev_3.5-6.5_1.pickle',  'cascades_15000ev_3.5-6.5_2.pickle',\n",
    "                  'cascades_15000ev_3.5-6.5_3.pickle',  'cascades_15000ev_3.5-6.5_4.pickle',\n",
    "                  'stracks_15000ev_3.5-6.5_1.pickle',   'stracks_15000ev_3.5-6.5_2.pickle', \n",
    "                  'stracks_15000ev_3.5-6.5_3.pickle',   'stracks_15000ev_3.5-6.5_4.pickle', \n",
    "                  'tracks_15000ev_3.5-6.5_1.pickle',    'tracks_15000ev_3.5-6.5_2.pickle', \n",
    "                  'tracks_15000ev_3.5-6.5_3.pickle',    'tracks_15000ev_3.5-6.5_4.pickle']\n",
    "            \n",
    "    events, records = pickle.load(open(os.path.join(path_to_events, load_array[it]), \"rb\"))\n",
    "    \n",
    "    print(f'{it} loaded!')\n",
    "    \n",
    "    return events, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pickle.dump(data_array, open(\"/dss/pone/pone_events/features_arrays/data_array_180k_k8_Rout_right.pickle\", \"wb\"))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dss/pone/pone_events/features_arrays/data_array_180k_k8_Rout.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/storage.py:222\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:938\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[1;32m    937\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[key]\n\u001b[0;32m--> 938\u001b[0m \u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pickle.dump(data_array, open(\"/dss/pone/pone_events/features_arrays/data_array_180k_k8_Rout_right.pickle\", \"wb\"))\n",
    "data_array = pickle.load(open(\"/dss/pone/pone_events/features_arrays/data_array_180k_k8_Rout.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(81722)\n",
    "\n",
    "indices = np.arange(len(data_array))\n",
    "random.shuffle(indices)\n",
    "\n",
    "\n",
    "shuffled_data = [data_array[i] for i in indices]\n",
    "split_test = int(len(shuffled_data)*0.9)\n",
    "training_data = shuffled_data[:split_test]\n",
    "split_val = int(len(training_data)*0.9)\n",
    "train_dataset = training_data[:split_val]\n",
    "val_dataset = training_data[split_val:]\n",
    "\n",
    "test_dataset = shuffled_data[split_test:]\n",
    "test_indices = indices[split_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Identity, ReLU, BatchNorm1d, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TAGConv, global_max_pool, BatchNorm, DynamicEdgeConv\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from torch_geometric.typing import NoneType\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n",
    "\n",
    "from graphnet.models.gnn.gnn import GNN\n",
    "from graphnet.components.layers import DynEdgeConv\n",
    "from nemesis.models.utils import calculate_xyz_homophily_POne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynEdge_modified(GNN):\n",
    "    def __init__(self, input_features, output_features, k = 15, features_subset = slice(12, 15), layer_size_scale=4):\n",
    "        \n",
    "        #Architecture configuration\n",
    "        c = layer_size_scale\n",
    "        l1, l2, l3, l4, l5, l6, l7 = (\n",
    "            input_features, c * 16 * 2, c * 32 * 2, c * 64 * 2, c * 32 * 2, c * 512 * 2, output_features)\n",
    "        \n",
    "        #Base class constructor\n",
    "        super().__init__(l1, l6)\n",
    "        \n",
    "        #First Layer\n",
    "        self.conv_add1 = DynEdgeConv(\n",
    "            torch.nn.Sequential(Linear(l1*2, l2),LeakyReLU(),Linear(l2, l3),LeakyReLU()),\n",
    "            aggr=\"max\",\n",
    "            nb_neighbors=k,\n",
    "            features_subset=features_subset)              \n",
    "          \n",
    "        #Second Layer  \n",
    "        self.conv_add2 = DynEdgeConv(\n",
    "            torch.nn.Sequential(Linear(l3*2, l4), LeakyReLU(), Linear(l4, l3), LeakyReLU()),\n",
    "            aggr=\"add\",\n",
    "            nb_neighbors=k,\n",
    "            features_subset=features_subset)\n",
    "        \n",
    "        #Third Layer\n",
    "        self.conv_add3 = DynEdgeConv(\n",
    "            torch.nn.Sequential(Linear(l3*2, l4),LeakyReLU(),Linear(l4, l3),LeakyReLU()),\n",
    "            aggr=\"max\",\n",
    "            nb_neighbors=k,\n",
    "            features_subset=features_subset)\n",
    "        \n",
    "        #Fourth Layer\n",
    "        self.conv_cat1 = DynEdgeConv(\n",
    "            torch.nn.Sequential(Linear(l3*2, l4),LeakyReLU(),Linear(l4, l3),LeakyReLU()),\n",
    "            aggr=\"add\",\n",
    "            nb_neighbors=k,\n",
    "            features_subset=features_subset)\n",
    "        \n",
    "        #Linear layers\n",
    "        self.nn1 = Linear(l3*4 + l1, l4)\n",
    "        self.nn2 = Linear(l4, l5)\n",
    "        self.nn3 = Linear(4*l5 + 3, l6)\n",
    "        self.lrelu = LeakyReLU()\n",
    "        \n",
    "        self.mlp = MLP([l6, l6/2, l6/8, l6/32, l7], dropout=0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        h_x, h_y, h_z = calculate_xyz_homophily_POne(x, edge_index, batch)\n",
    "        \n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "        \n",
    "        a, edge_index = self.conv_add1(x, edge_index, batch)\n",
    "        b, edge_index = self.conv_add2(a, edge_index, batch)\n",
    "        c, edge_index = self.conv_add3(b, edge_index, batch)\n",
    "        d, edge_index = self.conv_cat1(c, edge_index, batch)\n",
    "        \n",
    "        #Skip cat\n",
    "        x = torch.cat((x, a, b, c, d), dim=1)\n",
    "        \n",
    "        #Post-processing\n",
    "        x = self.nn1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.nn2(x)\n",
    "        \n",
    "        #Aggregation across nodes\n",
    "        a, _ = scatter_max(x, batch, dim=0)\n",
    "        b, _ = scatter_min(x, batch, dim=0)\n",
    "        c = scatter_sum(x, batch, dim=0)\n",
    "        d = scatter_mean(x, batch, dim=0)\n",
    "        \n",
    "        #Cat aggr and scalar feats\n",
    "        \n",
    "        x = torch.cat((a, b, c, d, h_x, h_y, h_z), dim=1)\n",
    "        \n",
    "        #Readout\n",
    "        \n",
    "        x = self.lrelu(x)\n",
    "        x = self.nn3(x)\n",
    "        \n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDynEdge_modified\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_subset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_size_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m count_parameters(model)\n",
      "Cell \u001b[0;32mIn [20], line 46\u001b[0m, in \u001b[0;36mDynEdge_modified.__init__\u001b[0;34m(self, input_features, output_features, k, features_subset, layer_size_scale)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn3 \u001b[38;5;241m=\u001b[39m Linear(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39ml5 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m, l6)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu \u001b[38;5;241m=\u001b[39m LeakyReLU()\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m([l6, l6\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, l6\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m, l6\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m, l7], dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "model = DynEdge_modified(15, 4, k = 8, features_subset = slice(12, 15), layer_size_scale=4)\n",
    "model.to(device)\n",
    "count_parameters(model)\n",
    "\n",
    "label_map = {0:\"Contained cascade\", 1:'Throughgoing Track', 2:\"Starts in detector\", 3:\"Rest of events\"}\n",
    "model, all_trains_acc, all_vals_acc = train_model(model, train_dataset, val_dataset, label_map, k=15, epochs=250, patience=20, print_step=1, batch_size=200, lr=0.001)\n",
    "plt.plot(np.linspace(0, len(all_trains_acc), len(all_trains_acc)), all_trains_acc, label=\"Training Accuracy\")\n",
    "plt.plot(np.linspace(0, len(all_vals_acc), len(all_vals_acc)), all_vals_acc, label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtest_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m fig1, miss_idx1, test_acc\u001b[38;5;241m=\u001b[39mmodel_evaluation(model, test_loader, all_events, all_records, all_labels, label_map, test_indices)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe test accuracy achieved is: \u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=250, shuffle=False)\n",
    "fig1, miss_idx1, test_acc=model_evaluation(model, test_loader, all_events, all_records, all_labels, label_map, test_indices)\n",
    "print('The test accuracy achieved is: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_models = \"/dss/pone/nemesis_trained_models/\"\n",
    "#model = torch.load(os.path.join(outpath, \"Dynmod_model_180k_R_75.pt\"))\n",
    "torch.save(model, os.path.join(outpath, \"DynMod_model_180k_R_out_B250_107E_L8.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ii in enumerate(data_array):\n",
    "    if i%1000 == 0:\n",
    "        print(ii.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import functools\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.8\"\n",
    "import sys\n",
    "sys.path.insert(0, '../olympus')\n",
    "sys.path.insert(0, '../hyperion')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "from olympus.event_generation.photon_propagation.norm_flow_photons import make_generate_norm_flow_photons, make_nflow_photon_likelihood\n",
    "from olympus.event_generation.photon_propagation.utils import sources_to_model_input\n",
    "from nemesis.event_generation.detector import Detector, make_line, make_triang\n",
    "from olympus.event_generation.event_generation import (\n",
    "    generate_cascade,\n",
    "    generate_cascades,\n",
    "    simulate_noise,\n",
    "    generate_realistic_track,\n",
    "    generate_realistic_tracks,\n",
    "    generate_realistic_tracks_test,\n",
    "    generate_realistic_starting_tracks,)\n",
    "from olympus.event_generation.lightyield import make_pointlike_cascade_source, make_realistic_cascade_source\n",
    "from olympus.event_generation.utils import sph_to_cart_jnp, proposal_setup\n",
    "\n",
    "#from olympus.plotting import plot_event\n",
    "from hyperion.medium import medium_collections\n",
    "from hyperion.constants import Constants\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "from jax import random\n",
    "from jax import numpy as jnp\n",
    "import json\n",
    "\n",
    "path_to_config = \"../hyperion/data/pone_config_optimistic.json\"\n",
    "config = json.load(open(path_to_config))[\"photon_propagation\"]\n",
    "ref_ix_f, sca_a_f, sca_l_f, _ = medium_collections[config[\"medium\"]]\n",
    "\n",
    "def c_medium_f(wl):\n",
    "    \"\"\"Speed of light in medium for wl (nm).\"\"\"\n",
    "    return Constants.BaseConstants.c_vac / ref_ix_f(wl)\n",
    "\n",
    "rng = np.random.RandomState(31338)\n",
    "oms_per_line = 20\n",
    "dist_z = 50 # m\n",
    "dark_noise_rate = 16 * 1e-5  # 1/ns\n",
    "side_len = 100 # m\n",
    "pmts_per_module = 16\n",
    "pmt_cath_area_r = 75E-3 / 2 # m\n",
    "module_radius = 0.21 # m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_events(v_x):\n",
    "    path_to_events = \"/dss/pone/pone_events\"\n",
    "    det, cascades, cascade_records = pickle.load(open(os.path.join(path_to_events, f\"cascades1500_vx{v_x}.pickle\"), \"rb\"))\n",
    "    det, tracks, track_records = pickle.load(open(os.path.join(path_to_events, f\"tracks1500_vx{v_x}.pickle\"), \"rb\"))\n",
    "    det, stracks, strack_records = pickle.load(open(os.path.join(path_to_events, f\"starting_tracks1500_vx{v_x}.pickle\"), \"rb\"))    \n",
    "    return det, cascades, cascade_records, tracks, track_records, stracks, strack_records\n",
    "    \n",
    "cascades_test, cascade_records_test = [], []\n",
    "tracks_test, track_records_test = [], []\n",
    "stracks_test, strack_records_test = [], []\n",
    "det_test = []\n",
    "v_x_all = []\n",
    "for it in range(1,21):\n",
    "    v_x_all.append(np.round(it*0.005, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da37430b6374283be6c89985c19b658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a29b0ecc7f94e8d8d881bfafca5d391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v_x_all = []\n",
    "for it in range(1,21):\n",
    "    v_x_all.append(np.round(it*0.005, 3))\n",
    "pbar = tqdm(total=len(v_x_all))\n",
    "\n",
    "for v_x in v_x_all:\n",
    "    det, cascades, cascade_records, tracks, track_records, stracks, strack_records = load_test()\n",
    "    cascades_test += cascades\n",
    "    cascade_records_test += cascade_records\n",
    "    tracks_test += tracks\n",
    "    track_records_test += track_records\n",
    "    stracks_test += stracks\n",
    "    strack_records_test += strack_records\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569cba603f8e475faf12693f8e6f1105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 speed loaded!\n",
      "0.01 speed loaded!\n",
      "0.015 speed loaded!\n",
      "0.02 speed loaded!\n",
      "0.025 speed loaded!\n",
      "0.03 speed loaded!\n",
      "0.035 speed loaded!\n",
      "0.04 speed loaded!\n",
      "0.045 speed loaded!\n",
      "0.05 speed loaded!\n",
      "0.055 speed loaded!\n",
      "0.06 speed loaded!\n",
      "0.065 speed loaded!\n",
      "0.07 speed loaded!\n",
      "0.075 speed loaded!\n",
      "0.08 speed loaded!\n",
      "0.085 speed loaded!\n",
      "0.09 speed loaded!\n",
      "0.095 speed loaded!\n",
      "0.1 speed loaded!\n"
     ]
    }
   ],
   "source": [
    "from nemesis.data_handling.event_loading import load_test\n",
    "\n",
    "det_test, cascades_test, cascade_records_test, tracks_test, track_records_test, stracks_test, strack_records_test = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_labels_test, track_labels_test, strack_labels_test = event_labelling(track_records_test, strack_records_test, cascade_records_test, det_hull=det.outer_cylinder)\n",
    "all_events_test = cascades_test + tracks_test + stracks_test\n",
    "all_records_test = cascade_records_test + track_records_test + strack_records_test\n",
    "all_labels_test = cascade_labels_test + track_labels_test + strack_labels_test\n",
    "all_records_test = pickle.load(open(\"/dss/pone/pone_events/features_arrays/test_data_array/all_records_test_90k_k8_R75_v_x.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89322b5f69654083a49273736d64e445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nemesis.evaluation.evaluation import model_evaluation, energy_division_loaders, energy_evaluation\n",
    "\n",
    "test_energy_loaders = energy_division_loaders(det, all_events_test, all_labels_test, all_records_test, num_divisions=11, k=15, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_energy_loaders, open(\"/dss/pone/pone_events/features_arrays/test_data_array/test_energy_loaders_90k_k15_Rout_B250_11divisions.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_labels_test, track_labels_test, strack_labels_test = event_labelling(track_records_test, strack_records_test, cascade_records_test, det_hull=(100,1000))\n",
    "all_events_test = cascades_test + tracks_test + stracks_test\n",
    "all_records_test = cascade_records_test + track_records_test + strack_records_test\n",
    "all_labels_test = cascade_labels_test + track_labels_test + strack_labels_test\n",
    "all_records_test = pickle.load(open(\"/dss/pone/pone_events/features_arrays/test_data_array/all_records_test_90k_k8_R75_v_x.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_energy_loaders = energy_division_loaders(det, all_events_test, all_labels_test, all_records_test, num_divisions=11, k=15, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_energy_loaders, open(\"/dss/pone/pone_events/features_arrays/test_data_array/test_energy_loaders_90k_k15_R100_B250_11divisions.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_labels_test, track_labels_test, strack_labels_test = event_labelling(track_records_test, strack_records_test, cascade_records_test, det_hull=(100,1000))\n",
    "all_events_test = cascades_test + tracks_test + stracks_test\n",
    "all_records_test = cascade_records_test + track_records_test + strack_records_test\n",
    "all_labels_test = cascade_labels_test + track_labels_test + strack_labels_test\n",
    "all_records_test = pickle.load(open(\"/dss/pone/pone_events/features_arrays/test_data_array/all_records_test_90k_k8_R75_v_x.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_energy_loaders, open(\"/dss/pone/pone_events/features_arrays/test_data_array/test_energy_loaders_90k_k15_R100_B250_11divisions.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch_geometric.loader.dataloader.DataLoader at 0x7fe6261e3c70>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe59ace2620>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe584d06ad0>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe58223ddb0>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe580341b70>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe57ecd4c10>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe57dc58fd0>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe57cf79b70>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe57c26ea10>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7fe551b40790>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_energy_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(data_array, open(\"/dss/pone/pone_events/features_arrays/data_array_test_90k_k8_R75_v_x.pickle\", \"wb\"))\n",
    "#pickle.dump(all_records_test_info, open(\"/dss/pone/pone_events/features_arrays/all_records_test_90k_k8_R75_v_x.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_test_info = pickle.load(open(\"/dss/pone/pone_events/features_arrays/all_records_test_90k_k8_R75_v_x.pickle\", \"rb\"))\n",
    "test_energy_loaders = energy_division_loaders(det, all_events_test, all_labels_test, all_records_test, num_divisions=11, k=15, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/dss/pone/nemesis_trained_models/Mar12/DynEdgeMod_180.0k_R100_k11_B200_L1_lr0.001.pt\")\n",
    "test_energy_loaders = pickle.load(open(\"/dss/pone/pone_events/features_arrays/test_data_array/test_energy_loaders_90k_k15_R75.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa518e80c834e4b8d36fbd50133ee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "first half done\n",
      "second half done\n",
      "The accuracies of the different energy divisions are [33.39610501702682, 36.014179683172706, 36.80731090803033, 37.82696177062374, 40.0, 42.359382777501246, 45.25139664804469, 41.37670196671709, 47.96680497925311]\n"
     ]
    }
   ],
   "source": [
    "fig, test_accuracies, misscls_idx = energy_evaluation(model, test_energy_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nemesis.node_features.feature_generation import get_features\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "def energy_division_loaders(det, all_events_test, all_labels_test:list, all_records_test:dict, num_divisions:int, k=8, batch_size=200,log_emin=None, log_emax=None):\n",
    "    #Create the energy bins\n",
    "    if log_emin and log_emax:\n",
    "        #Emin = np.power(10, log_emin)\n",
    "        #Emax = np.power(10, log_emax)\n",
    "        energy_divisions = np.logspace(log_emin, log_emax, num_divisions)\n",
    "        #energy_divisions = np.linspace(Emin, Emax, num_divisions)\n",
    "    else:\n",
    "        all_energies = [sub['energy'] for sub in all_records_test]\n",
    "        energy_divisions = np.linspace(np.min(all_energies), np.max(all_energies), num_divisions)\n",
    "\n",
    "    len_divisions = 0\n",
    "    #Create dict with the index of the events for each energy division\n",
    "    interval_dict = {}\n",
    "\n",
    "    for interval_nr in range(len(energy_divisions)-1):\n",
    "        interval_dict[f\"interval{interval_nr}\"] = []\n",
    "        count = 0\n",
    "        for nr,rc in enumerate(all_records_test):\n",
    "            if rc['energy'] >= energy_divisions[interval_nr] and rc['energy'] < energy_divisions[interval_nr+1]:\n",
    "                count+=1\n",
    "                #print(energy_divisions[interval_nr+1])\n",
    "                interval_dict[f\"interval{interval_nr}\"].append(nr)\n",
    "        len_divisions+=count\n",
    "\n",
    "    pbar = tqdm(total=len(all_labels_test))\n",
    "    test_energy_loaders = []\n",
    "\n",
    "    #Generate the test energy loaders for each energy division\n",
    "    for int_nr, interval in enumerate(interval_dict):\n",
    "        data_array = []\n",
    "        for ev in interval_dict[interval]:\n",
    "            features = get_features(det, all_events_test[ev])\n",
    "            valid = np.all(np.isfinite(features), axis=1)\n",
    "            features = features[valid]\n",
    "            x = torch.Tensor(features)\n",
    "\n",
    "            edge_index = knn_graph(x[:, [-1, -2, -3]], k=k, loop=False)\n",
    "            #data = transf(torch_geometric.data.Data(x, edge_index, y=torch.tensor([label], dtype=torch.int64)).to(device))\n",
    "            data = torch_geometric.data.Data(x, edge_index, y=torch.tensor(all_labels_test[ev], dtype=torch.int64)).to(device)\n",
    "            data_array.append(data)\n",
    "            pbar.update()\n",
    "        loader = DataLoader(data_array, batch_size, shuffle=False)\n",
    "        test_energy_loaders.append(loader)\n",
    "        \n",
    "    return test_energy_loaders\n",
    "\n",
    "def energy_evaluation(model, test_energy_loaders, plot_conf=True, label_map = {0:\"Contained cascade\", 1:'Throughgoing Track', 2:\"Starts in detector\", 3:\"Rest of events\"}):    \n",
    "\n",
    "    nplt = int(np.ceil(np.sqrt(len(test_energy_loaders))))\n",
    "    fig = plt.figure(figsize=(nplt * 4, nplt * 4))\n",
    "    pbar = tqdm(total=90000)\n",
    "    test_accuracies = []\n",
    "\n",
    "    ## Checking for predictions\n",
    "    with torch.no_grad():\n",
    "        for i, loader in enumerate(test_energy_loaders):\n",
    "            test_preds = []\n",
    "            test_truths = []\n",
    "            test_scores = []\n",
    "            all_preds_test_len = 0\n",
    "\n",
    "            for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "                out = model(data)\n",
    "                #print(out.shape)\n",
    "                test_pred = out.argmax(dim=1)\n",
    "                test_preds.append(test_pred)\n",
    "                test_truths.append(data.y)\n",
    "                test_scores.append(out)\n",
    "                all_preds_test_len+=len(data.y)\n",
    "                pbar.update()\n",
    "\n",
    "            test_preds = torch.cat(test_preds).cpu()\n",
    "            test_truths = torch.cat(test_truths).cpu()\n",
    "            test_scores = torch.cat(test_scores).cpu()\n",
    "            misscls_idx = np.atleast_1d(np.argwhere(test_preds != test_truths).ravel())\n",
    "            test_accuracy = 100 * (all_preds_test_len - len(misscls_idx)) / all_preds_test_len\n",
    "            all_preds = []\n",
    "            \n",
    "            for j in range(len(label_map)):\n",
    "                true_sel = test_truths == j\n",
    "                predictions = np.histogram(test_preds[true_sel], bins=np.arange(0, len(label_map)+1, 1))[0]\n",
    "                if predictions.sum() == 0:\n",
    "                    predictions = predictions\n",
    "                else:\n",
    "                    predictions = predictions / predictions.sum()\n",
    "                all_preds.append(predictions)\n",
    "\n",
    "            all_preds = np.vstack(all_preds)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            \n",
    "            if plot_conf:\n",
    "                ax = fig.add_subplot(nplt, nplt, i + 1)\n",
    "                ax1 = sns.heatmap(all_preds, cmap=plt.cm.Blues, annot=True, ax=ax)# xticklabels=list(label_map.values()), yticklabels=list(label_map.values()), ax=ax)\n",
    "                ax1.set_xlabel(\"Predicted Label\")\n",
    "                ax1.set_ylabel(\"True Label\")\n",
    "            #ax1.set_title()\n",
    "            #fig1 = ax.get_figure()\n",
    "            \n",
    "            ax, ax1 = 0,0\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    print('The accuracies of the different energy divisions are', test_accuracies)\n",
    "\n",
    "    return fig, test_accuracies, misscls_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
